:rsm:
  :title: Diagnostic Test Performance: ROC Curves
  :date: 2025-01-05

:author:
  :name: Dr. John Watson
  :affiliation: Department of Diagnostic Reasoning, 221B Medical College
::

:author:
  :name: Ebenezer Scrooge
  :affiliation: Department of Diagnostic Reasoning, 221B Medical College
::

:abstract:
  :keywords: {sensitivity, specificity, ROC curves, diagnostic tests}

Diagnostic tests require quantitative performance metrics to guide clinical decision-making. This paper introduces sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV), demonstrating how these measures relate through Bayes' theorem and depend on disease prevalence. The Receiver Operating Characteristic (ROC) curve visualizes the fundamental trade-off between sensitivity and specificity as diagnostic thresholds vary. Interactive visualizations illustrate how threshold selection and prevalence affect test performance.

/Example paper made with RSM Studio for Scroll Press./

::

## Performance Metrics

Diagnostic test evaluation begins with the 2×2 contingency table:

:table:

  :thead:
    :tr: : *Test Positive* : *Test Negative* ::
  ::

  :tbody:
    :tr: *Disease +* : TP : FN ::
    :tr: *Disease -* : FP : TN ::
  ::

::

Sensitivity (True Positive Rate) $\text{Sens} = \frac{TP}{TP + FN}$ measures the probability a diseased individual tests positive. Specificity (True Negative Rate) $\text{Spec} = \frac{TN}{TN + FP}$ measures the probability a healthy individual tests negative. Positive Predictive Value $\text{PPV} = \frac{TP}{TP + FP}$ gives the probability a positive test indicates true disease, while Negative Predictive Value $\text{NPV} = \frac{TN}{TN + FN}$ gives the probability a negative test indicates true health.

A key insight: sensitivity and specificity are intrinsic test properties, while PPV and NPV depend strongly on disease prevalence.

## The Prevalence Paradox

Bayes' theorem relates PPV to sensitivity, specificity, and prevalence:

$$
\text{PPV} = \frac{\text{Sens} \times \text{Prev}}{\text{Sens} \times \text{Prev} + (1-\text{Spec}) \times (1-\text{Prev})}
$$

Consider a test with 95% sensitivity and 95% specificity. At high prevalence (50%), PPV reaches approximately 95%. However, at low prevalence (1%), PPV drops to approximately 16%. In low-prevalence screening scenarios, even excellent tests produce many false positives relative to true positives, severely limiting PPV.

## Diagnostic Thresholds and ROC Curves

Many tests produce continuous results requiring a threshold for positive/negative classification. Lowering the threshold increases sensitivity (fewer false negatives) but decreases specificity (more false positives). Conversely, raising the threshold decreases sensitivity while increasing specificity. No single threshold is universally optimal—the choice depends on relative costs of false positives versus false negatives.

The Receiver Operating Characteristic (ROC) curve plots sensitivity versus (1 - specificity) as the threshold varies, visualizing performance across all operating points. The Area Under Curve (AUC) summarizes overall performance: AUC = 1.0 indicates a perfect test, AUC = 0.5 represents random guessing (diagonal line), AUC = 0.7-0.8 shows acceptable discrimination, and AUC = 0.9-1.0 demonstrates excellent discrimination. The AUC equals the probability that a randomly selected diseased individual has a higher test value than a randomly selected healthy individual.

## Interactive Exploration

The widget below demonstrates these relationships. The threshold slider moves along the ROC curve, showing how sensitivity and specificity change. The prevalence slider reveals how PPV and NPV change while sensitivity and specificity remain constant. The distribution separation control adjusts overlap between diseased and healthy populations. The visualization displays test result distributions, the 2×2 contingency table, calculated metrics, and the ROC curve with current operating point.

:html:
  :path: roc_curve_widget.html

  :caption: Interactive ROC curve explorer showing the relationship between sensitivity, specificity, PPV, NPV, and disease prevalence.

::

## Clinical Applications

Mammography screening demonstrates the prevalence paradox: with sensitivity ~80%, specificity ~95%, and prevalence ~0.5%, PPV reaches only ~10%. Most positive mammograms are false positives, requiring anxiety-inducing callbacks. The benefit of early detection must be weighed against these harms.

HIV testing uses high-sensitivity initial screening (>99%) to minimize missed cases, with confirmatory high-specificity testing to reduce false positives, balancing public health needs with individual accuracy.

COVID-19 rapid tests show high specificity (~99%) but moderate sensitivity (~75%). Positive results are reliable, but negative results don't rule out infection, especially early in disease course.

## Conclusion

Understanding diagnostic test performance requires recognizing what each metric measures: sensitivity captures how well the test finds disease, specificity how well it rules out disease, PPV what a positive result means, and NPV what a negative result means. Sensitivity and specificity are test properties independent of prevalence, while PPV and NPV depend critically on disease prevalence. ROC curves visualize the sensitivity-specificity trade-off across thresholds, with AUC summarizing overall accuracy. Optimal threshold selection balances the clinical consequences of false positives and false negatives for each specific context.

## References

:enumerate:

  :item: Pepe, M. S. (2003). /The Statistical Evaluation of Medical Tests for Classification and Prediction/. Oxford University Press.

  :item: Zhou, X. H., Obuchowski, N. A., & McClish, D. K. (2002). /Statistical Methods in Diagnostic Medicine/. Wiley.

::

---

/Example paper made with RSM Studio for Scroll Press./

::
